# Sequencing and data quality

**Sequencing and Data Quality Tutorial for Conservation Genomics**

This tutorial will guide you through the essential steps to evaluate and preprocess sequencing data, focusing on FASTQ file handling, quality assessment using **FastQC**, and data trimming with **Fastp**. These steps are critical for ensuring high-quality inputs for downstream analyses in conservation genomics.

## Section 1: Understanding Sequencing Output â€“ FASTQ Files

1.1) FASTQ files are the standard format for storing raw sequencing data. Each read in a FASTQ file consists of four lines:

1.	**Sequence Identifier**: Starts with @ and includes metadata.

2.	**Nucleotide Sequence**: The sequence of DNA bases (A, T, G, C).

3.	**Separator Line**: Starts with + and may repeat the identifier.

4.	**Quality Scores**: ASCII-encoded quality values corresponding to each base in the sequence.

![image](https://github.com/user-attachments/assets/5ffbd3ff-adb8-4fb0-81bc-4c66609771bd)

1.2) Understanding base quality (PHRED score) and the ASCII code
Each base in the FASTQ file has an associated quality score Q, which reflects the probability p that the base is incorrect. The formula to get Q is:

![image](https://github.com/user-attachments/assets/e8f0e2b2-5112-4ebc-888c-3126d8886ec0)

The values of Q can range from 0 to 93 (but the maximum score you will usually see is 40). To save space in a FASTQ file, each quality score is transformed into a single ASCII character.

![image](https://github.com/user-attachments/assets/5d0a8f1b-4a1d-4777-b8c9-a4325d242560)

Use basic UNIX commands to explore the data and get yourselves familiar with the FASTQ format. 

1) How many reads do we have for each individual? Use the command line below to count the lines that start with the specified header characters.

```
grep -c "^@A" fastq.fastq
```

2) What are the read lengths? The `akw` command below determines the distribution of read lengths in a FASTQ file by counting how many reads are of each specific length.

```
awk '(NR%4==2) {print length($0)}' your_data.fastq | sort | uniq -c
```

4) Getting familiar with quality values:
Use the ASCII table below to get the numerical value for the ASCII character *. It is 42. The convention is to subtract 33 (phred33 encoded), which makes 9.
Is this a good quality? We change the formal Q = -10 log 10 p to p = 10 (Q/-10). Now that you know how to convert quality values fill out the following table:

![image](https://github.com/user-attachments/assets/eb398136-18e2-44b7-a4d1-74d2b9de937f)


|Quality in fastq |	Q in decimal |	p|
|---|---|---|
| I |	40	| 0.0001 |
| *| 9|  |
| | 23 | |
**Exercise: View a FASTQ File**

1.	Navigate to the directory containing your FASTQ file:

```bash
cd /path/to/your/data
```

2.	Use less to inspect the file:

```bash
less your_data.fastq
```

Scroll through the lines to identify the structure described above.

3.	Count the number of reads (each read has four lines):

```bash
wc -l your_data.fastq
```

Divide the total number of lines by 4 to calculate the number of reads.

## Section 2: Evaluating Data Quality with FastQC

FastQC is a widely used tool for evaluating sequencing data quality. It generates a detailed report that includes metrics like:

- **Per Base Quality**: Average quality scores across the length of reads.
- **GC Content**: The percentage of G and C bases.
- **Duplicate Levels**: Identifies potential PCR duplicates.
- **Adapter Content**: Checks for residual sequencing adapters.


**Installing FastQC**

If FastQC is not installed on your HPC, load it using a module:

```bash
module load fastqc
```

**Running FastQC**

1.	Create a directory for output:

```bash
mkdir fastqc_reports
```

2.	Run FastQC on a single file:

```bash
fastqc your_data.fastq -o fastqc_reports
```

3.	Run FastQC on all FASTQ files in the directory:

```bash
fastqc *.fastq -o fastqc_reports
```

**Viewing the Results**

- Navigate to the fastqc_reports directory and download the *_fastqc.html files to your local machine using scp or rsync.
- Open the HTML file in a browser to examine the quality metrics.

**Section 3: Trimming and Filtering with Fastp**

Fastp is an efficient tool for quality control and preprocessing. It can:

- Trim low-quality bases.
- Remove adapters.
- Filter short reads.
- Generate QC reports.

**Installing Fastp**

Load Fastp on your HPC:

```bash
module load fastp
```

**Running Fastp**

1.	Trim a single file:

```bash
fastp -i your_data.fastq -o your_data_trimmed.fastq
```

Key parameters:

- `-i`: Input file.
- `-o`: Output file.

2.	Trim paired-end reads:

```bash
fastp -i read1.fastq -I read2.fastq -o read1_trimmed.fastq -O read2_trimmed.fastq
```

3.	Include a summary report:

```bash
fastp -i your_data.fastq -o your_data_trimmed.fastq --html fastp_report.html
```

4.	Trim all files in a directory using a loop:

```bash
for file in *.fastq; do
fastp -i "$file" -o "${file%.fastq}_trimmed.fastq"
done
```

**Section 4: Comparing Pre- and Post-Trimming Quality**

**Quality Check on Trimmed Data**

- Rerun FastQC on the trimmed data to confirm improvements:

```bash
fastqc your_data_trimmed.fastq -o fastqc_reports
```

- Compare the pre- and post-trimming reports to evaluate changes in:
- Base quality scores.
- Adapter content.
- Read length distribution.

**Section 5: Practical Workflow**

**Step 1: Initial Inspection**

- Use `wc` and `less` to verify the FASTQ structure and estimate dataset size.

**Step 2: Evaluate Raw Data Quality**

- Run FastQC and interpret the reports for potential issues (e.g., adapters, low-quality bases).

**Step 3: Perform Quality Trimming**

- Use Fastp to remove adapters, trim low-quality bases, and filter short reads.

**Step 4: Re-evaluate Trimmed Data**

- Run FastQC again on trimmed files to confirm quality improvement.

**Section 6: Best Practices for Data Quality**

1.	**Retain Metadata**: Keep logs and summaries from FastQC and Fastp for reproducibility.

2.	**Optimize Parameters**: Tailor Fastp settings (e.g., trimming thresholds) based on FastQC reports.

3.	**Batch Processing**: Use loops or job arrays for large datasets.

4.	**Save Outputs**: Maintain a structured directory with raw, trimmed, and quality-checked data.

**Additional Commands for Troubleshooting**

1.	**Check File Size**:

```bash
ls -lh your_data.fastq
```

2.	**Count Adapters**:

```bash
grep -c "adapter_sequence" your_data.fastq
```

3.	**Extract Read Lengths**:

```bash
awk '(NR%4==2) {print length($0)}' your_data.fastq | sort | uniq -c
```

### Section 7: About demultiplexing sequencing data

**What is Demultiplexing?**

Demultiplexing is the process of separating sequencing reads from a single FASTQ file (or paired FASTQ files) into individual files for each sample. This is achieved using **barcodes**â€”short DNA sequences unique to each sample.

<aside>
ðŸ’¡

The dataset we are going to work is already demultiplexed, but is good practice to check if this was already performed by the sequencing facility. 

</aside>

Demultiplexing ensures:

- Accurate assignment of reads to samples.
- Removal of barcode sequences from the data.

**Tools for Demultiplexing**

Common tools for demultiplexing include:

1.	**bcl2fastq**: Often used for Illumina data.

2.	**Fastp**: Includes demultiplexing capabilities.

3.	**AdapterRemoval**: Useful for small datasets.

4.	**Custom Scripts**: For specialized needs.

**Demultiplexing with bcl2fastq**

**1. Overview**

bcl2fastq converts Illuminaâ€™s raw BCL files into demultiplexed FASTQ files. It also trims barcodes and adapters during this process.

**2. Prerequisites**

- Access to raw BCL files.
- A sample sheet (SampleSheet.csv) containing sample names, barcodes, and metadata.

**3. Running bcl2fastq**

1.	Navigate to the directory with BCL files:

```bash
cd /path/to/bcl/files
```

2.	Run bcl2fastq:

```bash
bcl2fastq --runfolder-dir . --output-dir /path/to/output
```

Key parameters:

â€¢	`--runfolder-dir`: Directory with BCL files.

â€¢	`--output-dir`: Directory for FASTQ output.

3.	Check the output directory for demultiplexed FASTQ files, organized by sample.

**Demultiplexing with Fastp**

If your sequencing output is already in FASTQ format but needs demultiplexing:

**1. Prepare a Barcode File**

Create a text file with sample names and their barcodes:

```bash
sample1Â  Â  ATCG
sample2Â  Â  CGTA
sample3Â  Â  TTAA
```

**2. Run Fastp**

Demultiplex paired-end reads:

```bash
fastp -i input_R1.fastq -I input_R2.fastq \
--demultiplex_barcode_file barcodes.txt \
--out1 demultiplexed_R1.fastq \
--out2 demultiplexed_R2.fastq
```

**Demultiplexing Using Custom Scripts**

For more control, you can use Python or Bash scripts to demultiplex reads based on barcodes. For example:

```bash
grep -A 3 '^@.*ATCG' input.fastq > sample1.fastq
grep -A 3 '^@.*CGTA' input.fastq > sample2.fastq
```

**Quality Control After Demultiplexing**

Once reads are separated:

1.	**Run FastQC**:

```bash
fastqc sample1.fastq -o qc_reports
```

2.	**Trim Adapters** (if not already done during demultiplexing):

```bash
fastp -i sample1.fastq -o sample1_trimmed.fastq
```

**Practical Workflow for Demultiplexing**

1.	**Check Raw Data**: Identify whether the data is in BCL or FASTQ format.

2.	**Choose a Demultiplexing Tool**: Use bcl2fastq for BCL files or fastp/custom scripts for FASTQ files.

3.	**Verify Results**: Confirm that demultiplexed FASTQ files match the sample sheet.

4.	**Run Quality Control**: Use tools like FastQC to assess the quality of the demultiplexed reads.

**Best Practices for Demultiplexing**

1.	**Validate Sample Sheet**: Ensure barcode sequences in the sample sheet are correct.

2.	**Inspect Outputs**: Check the number of reads per sample to identify potential issues.

3.	**Automate**: Use pipelines or scripts for batch processing of large datasets.

4.	**Document Everything**: Keep records of commands, sample sheets, and log files for reproducibility.

Demultiplexing is a critical step in sequencing workflows, enabling accurate downstream analyses. With these tools and strategies, you can efficiently process and prepare your data for quality control and analysis.

### **Section 8: Summary**

By following this workflow, you ensure your sequencing data is high-quality and ready for downstream analysis. These tools and techniques are foundational in conservation genomics, helping to maximize the reliability and accuracy of your results.
